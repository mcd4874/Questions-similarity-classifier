{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'questions.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix,log_loss, accuracy_score\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import os\n",
    "print(os.listdir(\"./input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "# use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/questions.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343695, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train,df_test = train_test_split(df,test_size=0.15, random_state=0)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosim(row):\n",
    "    a = cv.transform([row.question1])\n",
    "    b = cv.transform([row.question2])\n",
    "    return cosine_similarity(a,b).ravel()[0]\n",
    "def makePredict(row):\n",
    "    if (row.cosim > 0.5):\n",
    "        return 1\n",
    "    return 0\n",
    "def generatePrediction(df):\n",
    "    df['cosim'] =  df.apply (lambda row: calculate_cosim(row), axis=1)\n",
    "    df['predict'] =  df.apply (lambda row: makePredict(row), axis=1)\n",
    "    return df\n",
    "\n",
    "def evaluate_acc(actual,predict):\n",
    "    return accuracy_score(actual, predict)\n",
    "    \n",
    "# df_train['cosim'] =  df_train.apply (lambda row: calculate_cosim(row), axis=1)\n",
    "# df_test['cosim'] =  df_test.apply (lambda row: calculate_cosim(row), axis=1)\n",
    "# df_train['predict'] =  df_train.apply (lambda row: makePredict(row), axis=1)\n",
    "# df_test['predict'] =  df_test.apply (lambda row: makePredict(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(df_train.is_duplicate, df_train.predict)\n",
    "# df_test.head()\n",
    "# accuracy_score(df_test.is_duplicate, df_test.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we start to train word embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location,retrain):\n",
    "    \"\"\"Returns trained word2vec\n",
    "    \n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location) and not retrain :\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    # Replace punctuation characters with spaces\n",
    "    filters = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(687390,)\n"
     ]
    }
   ],
   "source": [
    "# print(df_train['question1'].shape)\n",
    "corpus_train = np.concatenate((df_train['question1'], df_train['question2']), axis=0)\n",
    "# print(corpus_train.shape)\n",
    "\n",
    "# w2vec = get_word2vec(\n",
    "#     MySentences(\n",
    "#        corpus_train\n",
    "#     ),\n",
    "#     'w2vmodel.model',\n",
    "#     False\n",
    "# )\n",
    "# mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\n",
    "print(corpus_train.shape)\n",
    "mean_embedding_vectorizer= CountVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        preprocessor=clean_text\n",
    "    ).fit(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343695, 82010)\n",
      "(343695, 82010)\n",
      "(60653, 82010)\n",
      "(60653, 82010)\n"
     ]
    }
   ],
   "source": [
    "# generate embedding vector\n",
    "\n",
    "\n",
    "question1_vectors = mean_embedding_vectorizer.transform(df_train['question1'])\n",
    "question2_vectors = mean_embedding_vectorizer.transform(df_train['question2'])\n",
    "\n",
    "question1_test_vectors = mean_embedding_vectorizer.transform(df_test['question1'])\n",
    "question2_test_vectors = mean_embedding_vectorizer.transform(df_test['question2'])\n",
    "print(question1_vectors.shape)\n",
    "print(question2_vectors.shape)\n",
    "print(question1_test_vectors.shape)\n",
    "print(question2_test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to generate features\n",
    "def sqrt_sum(a,b):\n",
    "    return np.sqrt(np.sum((a-b)**2, axis=1))\n",
    "\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    if (np.linalg.norm(vector) == 0):\n",
    "        return np.zeros(vector.shape)\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    if (v1_u.all() == 0 or v2_u.all() == 0):\n",
    "        return 0\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "def angle_wise_matrix(matrix1, matrix2):\n",
    "    size = matrix1.shape[0]\n",
    "    result = np.zeros(size)\n",
    "    for i in range (size):\n",
    "        result[i] = angle_between(matrix1[i],matrix2[i])\n",
    "    return result\n",
    "def calculate_cosim_vectors(matrix1,matrix2):\n",
    "    result = np.zeros(matrix1.shape[0])\n",
    "    for i in range(matrix1.shape[0]):        \n",
    "        result[i] = cosine_similarity(matrix1[i:i+1],matrix2[i:i+1])\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cosim = calculate_cosim_vectors(question1_vectors,question2_vectors)\n",
    "test_cosim = calculate_cosim_vectors(question1_test_vectors,question2_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343695,)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics.pairwise\n",
    "def matrix_sqrt_sum(matrix1,matrix2):\n",
    "    result = np.zeros(matrix1.shape[0])\n",
    "    for i in range(matrix1.shape[0]):  \n",
    "        result[i]= sklearn.metrics.pairwise.pairwise_distances(matrix1[i],matrix2[i])\n",
    "    return result\n",
    "dist = matrix_sqrt_sum(question1_vectors,question2_vectors)\n",
    "print(dist.shape)\n",
    "# angles = angle_wise_matrix(question1_vectors,question2_vectors)\n",
    "dist_test = matrix_sqrt_sum(question1_test_vectors,question2_test_vectors)\n",
    "# angles_test = angle_wise_matrix(question1_test_vectors,question2_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343695, 82010)\n",
      "(343695, 82010)\n",
      "(343695, 164020)\n",
      "(343695, 164020)\n"
     ]
    }
   ],
   "source": [
    "print(question1_vectors.shape)\n",
    "print(question2_vectors.shape)\n",
    "from scipy.sparse import coo_matrix, vstack, hstack\n",
    "vec_representation = hstack([question1_vectors, question2_vectors])\n",
    "vec_representation_test = hstack([question1_test_vectors, question2_test_vectors])\n",
    "print(vec_representation.shape)\n",
    "# vec_representation = np.concatenate([question1_vectors, question2_vectors],axis=1)\n",
    "# vec_representation_test = np.concatenate([question1_test_vectors, question2_test_vectors],axis=1)\n",
    "print(vec_representation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343695, 2)\n"
     ]
    }
   ],
   "source": [
    "# print(\"nan in dist : \",np.isnan(dist).any())\n",
    "# print(\"nan in angle : \",np.isnan(angles).any())\n",
    "\n",
    "# print(\"nan in test dist : \",np.isnan(dist_test).any())\n",
    "# print(\"nan in test angle : \",np.isnan(angles_test).any())\n",
    "# x_input =  np.stack([dist, angles,train_cosim],axis=1)\n",
    "x_input =  np.stack([dist, train_cosim],axis=1)\n",
    "# x_input =  np.stack([dist, angles],axis=1)\n",
    "y_input = df_train['is_duplicate'].values\n",
    "\n",
    "# x_test =  np.stack([dist_test, angles_test,test_cosim],axis=1)\n",
    "x_test =  np.stack([dist_test, test_cosim],axis=1)\n",
    "# x_test =  np.stack([dist_test, angles_test],axis=1)\n",
    "y_test = df_test['is_duplicate'] .values\n",
    "\n",
    "print(x_input.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = len(y_input)\n",
    "# y_input= y_input.reshape(-1,1)\n",
    "# y_test = y_test.reshape(-1,1)\n",
    "# print(x_input.shape)\n",
    "# print(x_test.shape)\n",
    "# print(y_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "def get_report(x_test,y_test,model):\n",
    "    predict = model.predict(x_test)\n",
    "    print(classification_report(y_test, predict))\n",
    "def generate_model(x_input,y_input,model,savePath):\n",
    "    x_input,x_valid,y_input,y_valid = train_test_split(x_input, y_input, test_size=0.2, random_state=4242)\n",
    "    model.fit(x_input,y_input)\n",
    "    get_report(x_valid,y_valid,model)\n",
    "#     predict = model.predict(x_valid)\n",
    "#     print(classification_report(y_test, y_valid))\n",
    "    with open(savePath, 'wb') as file:\n",
    "        pickle.dump(model, file, -1)\n",
    "    return model\n",
    "def load_model(loadPath):\n",
    "    with open(loadPath, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343695, 164020)\n"
     ]
    }
   ],
   "source": [
    "# clf = SVC(gamma='auto')\n",
    "rf= RandomForestClassifier()\n",
    "print(vec_representation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamduong/anaconda3/envs/genism/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83     43264\n",
      "           1       0.73      0.65      0.69     25475\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     68739\n",
      "   macro avg       0.77      0.75      0.76     68739\n",
      "weighted avg       0.78      0.78      0.78     68739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = generate_model(x_input,y_input,rf,\"rf.pkl\")\n",
    "model = generate_model(vec_representation,y_input,rf,\"rf_vector.pkl\")\n",
    "# model = generate_model(vec_representation,y_input,clf,\"svm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_report(x_test,y_test,model)\n",
    "get_report(vec_representation_test,y_test,model)\n",
    "# get_report(vec_representation_test,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_time_step_data(input,timeStep):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323478, 2, 1) (323478, 1) (80870, 2, 1) (80870, 1)\n"
     ]
    }
   ],
   "source": [
    "# transform shape for LSTM (require 3D array)\n",
    "# time_step = 1\n",
    "# input_features = x_input.shape[1]\n",
    "# output_festures = y_input.shape[1]\n",
    "# print(input_features)\n",
    "# print(output_festures)\n",
    "\n",
    "\n",
    "x_input = x_input.reshape(int(x_input.shape[0]),input_features,1)\n",
    "x_test = x_test.reshape(int(x_test.shape[0]),input_features,1)\n",
    "\n",
    "# y_input = y_input.reshape(int(y_input.shape[0]),time_step,output_festures)\n",
    "# y_test = y_test.reshape(int(y_test.shape[0]),time_step,output_festures)\n",
    "\n",
    "print(x_input.shape, y_input.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "def create_model(n_batch,features,time_step = 2):\n",
    "    print(features)\n",
    "    print(time_step)\n",
    "    model = Sequential()\n",
    "#     model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "    model.add(LSTM(100,input_shape=(time_step,features)))\n",
    "    model.add(Dense(256,name='FC1', activation = 'relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    model.add(Dense(1,name='out_layer', activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 66,913\n",
      "Trainable params: 66,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model(x_input.shape[0],1, 2)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/williamduong/anaconda3/envs/genism/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 291130 samples, validate on 32348 samples\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_input,y_input,batch_size=128,epochs=5,\n",
    "          validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(x_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
